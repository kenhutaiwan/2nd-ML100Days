{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle上的貓狗大戰分類題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預防錯誤： OMP: Error #15: Initializing libiomp5.dylib, but found libiomp5.dylib already initialized.\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料整理，從解開的訓練集中，用狗猫各1000張做訓練集，各500張做驗證集，各500張做測試集\n",
    "# 照片用cat.NUMBER.jpg、dog.NUMBER.jpg命名，NUMBER from 0 to 12499\n",
    "\n",
    "import os, shutil\n",
    "original_dataset_dir = '/Users/ken/Downloads/dogs-vs-cats/train/'\n",
    "base_dir = '/Users/ken/Downloads/cats_and_dogs_small'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "# 在新的資料存放目錄下新增三個資料夾：train、validation、test\n",
    "train_dir = os.path.join(base_dir, 'train') \n",
    "os.mkdir(train_dir) \n",
    "validation_dir = os.path.join(base_dir, 'validation') \n",
    "os.mkdir(validation_dir) \n",
    "test_dir = os.path.join(base_dir, 'test') \n",
    "os.mkdir(test_dir)\n",
    "\n",
    "# 在上述三個資料集目錄下再新增dogs、cats兩個資料夾\n",
    "train_cats_dir = os.path.join(train_dir, 'cats') \n",
    "os.mkdir(train_cats_dir)\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs') \n",
    "os.mkdir(train_dogs_dir)\n",
    "\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats') \n",
    "os.mkdir(validation_cats_dir)\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs') \n",
    "os.mkdir(validation_dogs_dir)\n",
    "\n",
    "test_cats_dir = os.path.join(test_dir, 'cats') \n",
    "os.mkdir(test_cats_dir)\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs') \n",
    "os.mkdir(test_dogs_dir)\n",
    "\n",
    "# 複製1000張貓狗照做訓練集\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)] \n",
    "for fname in fnames:\n",
    "  src = os.path.join(original_dataset_dir, fname) \n",
    "  dst = os.path.join(train_cats_dir, fname) \n",
    "  shutil.copyfile(src, dst)\n",
    "\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)] \n",
    "for fname in fnames:\n",
    "  src = os.path.join(original_dataset_dir, fname) \n",
    "  dst = os.path.join(train_dogs_dir, fname) \n",
    "  shutil.copyfile(src, dst)\n",
    "\n",
    "# 複製500張貓狗照做驗證集\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000,1500)] \n",
    "for fname in fnames:\n",
    "  src = os.path.join(original_dataset_dir, fname) \n",
    "  dst = os.path.join(validation_cats_dir, fname) \n",
    "  shutil.copyfile(src, dst)\n",
    "\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000,1500)] \n",
    "for fname in fnames:\n",
    "  src = os.path.join(original_dataset_dir, fname) \n",
    "  dst = os.path.join(validation_dogs_dir, fname) \n",
    "  shutil.copyfile(src, dst)\n",
    "\n",
    "# 複製500張貓狗照做測試集\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500,2000)] \n",
    "for fname in fnames:\n",
    "  src = os.path.join(original_dataset_dir, fname) \n",
    "  dst = os.path.join(test_cats_dir, fname) \n",
    "  shutil.copyfile(src, dst)\n",
    "\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500,2000)] \n",
    "for fname in fnames:\n",
    "  src = os.path.join(original_dataset_dir, fname) \n",
    "  dst = os.path.join(test_dogs_dir, fname) \n",
    "  shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ken/anaconda3/envs/ai100-2/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# 定義網路\n",
    "\n",
    "from keras import layers \n",
    "from keras import models\n",
    "\n",
    "# 照片大小並不一致，隨意選兩張，有375x299、500x374，還有更多種Size。這裡任意以150x150做為輸入照片大小，訓練前要先resize\n",
    "model = models.Sequential() \n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3))) \n",
    "model.add(layers.MaxPooling2D((2, 2))) \n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu')) \n",
    "model.add(layers.MaxPooling2D((2, 2))) \n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu')) \n",
    "model.add(layers.MaxPooling2D((2, 2))) \n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu')) \n",
    "model.add(layers.MaxPooling2D((2, 2))) \n",
    "model.add(layers.Flatten()) \n",
    "model.add(layers.Dense(512, activation='relu')) \n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編繹網路 \n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# 讀取JPEG檔, 轉成RGB值後做Normalization，並resize成150x150\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255) \n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, \n",
    "  target_size=(150, 150), batch_size=20, class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir, \n",
    "  target_size=(150, 150), batch_size=20, class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ken/anaconda3/envs/ai100-2/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 82s 824ms/step - loss: 0.6866 - acc: 0.5365 - val_loss: 0.6626 - val_acc: 0.5870\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 73s 726ms/step - loss: 0.6360 - acc: 0.6270 - val_loss: 0.6357 - val_acc: 0.6070\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 72s 723ms/step - loss: 0.5894 - acc: 0.6935 - val_loss: 0.6001 - val_acc: 0.6770\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 73s 731ms/step - loss: 0.5541 - acc: 0.7070 - val_loss: 0.6059 - val_acc: 0.6710\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 72s 725ms/step - loss: 0.5232 - acc: 0.7430 - val_loss: 0.5815 - val_acc: 0.6810\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 73s 735ms/step - loss: 0.4949 - acc: 0.7640 - val_loss: 0.5765 - val_acc: 0.6810\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.4819 - acc: 0.7715 - val_loss: 0.5610 - val_acc: 0.7070\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 80s 800ms/step - loss: 0.4529 - acc: 0.7890 - val_loss: 0.5655 - val_acc: 0.7060\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 80s 797ms/step - loss: 0.4344 - acc: 0.7895 - val_loss: 0.5959 - val_acc: 0.6810\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 80s 801ms/step - loss: 0.4002 - acc: 0.8225 - val_loss: 0.5555 - val_acc: 0.7230\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 78s 778ms/step - loss: 0.3782 - acc: 0.8355 - val_loss: 0.5743 - val_acc: 0.7230\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 81s 812ms/step - loss: 0.3496 - acc: 0.8505 - val_loss: 0.5687 - val_acc: 0.7200\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.3265 - acc: 0.8605 - val_loss: 0.5702 - val_acc: 0.7300\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 81s 814ms/step - loss: 0.3072 - acc: 0.8770 - val_loss: 0.5410 - val_acc: 0.7400\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 86s 858ms/step - loss: 0.2802 - acc: 0.8830 - val_loss: 0.5659 - val_acc: 0.7290\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 76s 755ms/step - loss: 0.2528 - acc: 0.9000 - val_loss: 0.6974 - val_acc: 0.7080\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 78s 776ms/step - loss: 0.2386 - acc: 0.9065 - val_loss: 0.5957 - val_acc: 0.7380\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 84s 840ms/step - loss: 0.2117 - acc: 0.9225 - val_loss: 0.5889 - val_acc: 0.7420\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 79s 789ms/step - loss: 0.1932 - acc: 0.9345 - val_loss: 0.6900 - val_acc: 0.7380\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.1837 - acc: 0.9335 - val_loss: 0.5919 - val_acc: 0.7440\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 89s 892ms/step - loss: 0.1569 - acc: 0.9490 - val_loss: 0.6528 - val_acc: 0.7390\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.1438 - acc: 0.9525 - val_loss: 0.6808 - val_acc: 0.7330\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 79s 789ms/step - loss: 0.1216 - acc: 0.9595 - val_loss: 0.7434 - val_acc: 0.7440\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 76s 760ms/step - loss: 0.1073 - acc: 0.9650 - val_loss: 0.7474 - val_acc: 0.7340\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 73s 733ms/step - loss: 0.0949 - acc: 0.9700 - val_loss: 0.7566 - val_acc: 0.7410\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 73s 730ms/step - loss: 0.0821 - acc: 0.9815 - val_loss: 0.8392 - val_acc: 0.7300\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 73s 731ms/step - loss: 0.0699 - acc: 0.9810 - val_loss: 0.8634 - val_acc: 0.7440\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 73s 730ms/step - loss: 0.0572 - acc: 0.9840 - val_loss: 0.8401 - val_acc: 0.7470\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 74s 736ms/step - loss: 0.0511 - acc: 0.9880 - val_loss: 0.8412 - val_acc: 0.7550\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 73s 728ms/step - loss: 0.0503 - acc: 0.9855 - val_loss: 0.9264 - val_acc: 0.7370\n"
     ]
    }
   ],
   "source": [
    "# 訓練模型。這裡改用fit_generator，使用generator做資料來源\n",
    "\n",
    "history = model.fit_generator(train_generator, \n",
    "                              steps_per_epoch=100, \n",
    "                              epochs=30, \n",
    "                              validation_data=validation_generator, \n",
    "                              validation_steps=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
