{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此函數會幫我們把多張影像畫成一張多宮格圖\n",
    "def img_combine(img, ncols=8, size=1, path=False):\n",
    "    from math import ceil\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    nimg = len(img)\n",
    "    nrows = int(ceil(nimg/ncols))\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, sharex=True, sharey=True, figsize=(ncols*size,nrows*size))\n",
    "    if nrows == 0:\n",
    "        return\n",
    "    elif ncols == 1:\n",
    "        for r, ax in zip(np.arange(nrows), axes):\n",
    "            nth=r\n",
    "            if nth < nimg:\n",
    "                ax.imshow(img[nth], cmap='rainbow', vmin=0, vmax=1)\n",
    "                \n",
    "            ax.set_axis_off()\n",
    "    elif nrows == 1:\n",
    "        for c, ax in zip(np.arange(ncols), axes):\n",
    "            nth=c\n",
    "            if nth < nimg:\n",
    "                ax.imshow(img[nth], cmap='rainbow', vmin=0, vmax=1)\n",
    "            ax.set_axis_off()\n",
    "    else:\n",
    "        for r, row in zip(np.arange(nrows), axes):\n",
    "            for c, ax in zip(np.arange(ncols), row):\n",
    "                nth=r*ncols+c\n",
    "                if nth < nimg:\n",
    "                    ax.imshow(img[nth], cmap='rainbow', vmin=0, vmax=1)\n",
    "                ax.set_axis_off()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取 Cifar-10 資料集\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取前 32 張圖片做視覺化\n",
    "images = x_train[:32]\n",
    "img_combine(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 ImageDataGenerator，並指定我們要做資料增強的數值範圍\n",
    "data_generator = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意！！ ImageDataGenerator 是一個 Generator (生成器)! 對 Generator 不熟悉的同學請回到 Day098 做複習。\n",
    "# 使用 .flow 後，就會對我們的影像進行增強，再 call next 取出 generator 的圖像。(shuffle=False 因為我們希望圖像的順序不要改變，方便觀察。實際訓練時預設是 shuffle=True) \n",
    "augmented_iamges = next(data_generator.flow(images, shuffle=False))\n",
    "img_combine(augmented_iamges.astype(\"int\")) # 注意在訓練時神經網路時，圖像資料必須要是 float32，但在做視覺化時要轉為 int 才能順利畫圖。所以為了畫圖才把資料轉為 int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因為隨機性的關係，所以一樣的圖像再經過一次 generator 後的結果不一定相同\n",
    "augmented_iamges = next(data_generator.flow(images, shuffle=False))\n",
    "img_combine(augmented_iamges.astype(\"int\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請使用 ImageDataGenerator 來進行 Cifar-10 資料集的訓練，並觀察不同的圖像增強方法是否會顯著影響訓練結果\n",
    "\n",
    "- 資料增強應該要在圖像標準化之前完成 (e.g. 除以 255、減去平均值)\n",
    "- 先對資料做 train/test split 後再做資料增強\n",
    "- Keras本身就有提供這個題目的範例：[Train a simple deep CNN on the CIFAR10 small images dataset](https://keras.io/examples/cifar10_cnn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import os\n",
    "\n",
    "# 預防錯誤： OMP: Error #15: Initializing libiomp5.dylib, but found libiomp5.dylib already initialized.\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128 # batch 的大小，如果出現 OOM error，請降低這個值\n",
    "num_classes = 10 # 類別的數量，Cifar 10 共有 10 個類別\n",
    "epochs = 10 # 訓練的 epochs 數量\n",
    "\n",
    "# 讀取資料並檢視\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# 對 label 進行 one-hot encoding (y_trian 原本是純數字)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "data_generator.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# 資料前處理: 變為 float32 並標準化\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "390/390 [==============================] - 151s 387ms/step - loss: 1.9206 - acc: 0.2951 - val_loss: 1.5903 - val_acc: 0.4018\n",
      "Epoch 2/10\n",
      "390/390 [==============================] - 152s 391ms/step - loss: 1.6247 - acc: 0.4101 - val_loss: 1.2987 - val_acc: 0.5317\n",
      "Epoch 3/10\n",
      "390/390 [==============================] - 147s 377ms/step - loss: 1.4842 - acc: 0.4678 - val_loss: 1.1646 - val_acc: 0.5779\n",
      "Epoch 4/10\n",
      "390/390 [==============================] - 147s 378ms/step - loss: 1.3916 - acc: 0.5006 - val_loss: 1.2024 - val_acc: 0.5614\n",
      "Epoch 5/10\n",
      "390/390 [==============================] - 149s 381ms/step - loss: 1.3265 - acc: 0.5261 - val_loss: 1.1243 - val_acc: 0.5921\n",
      "Epoch 6/10\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 1.2797 - acc: 0.5444 - val_loss: 1.0594 - val_acc: 0.6263\n",
      "Epoch 7/10\n",
      "390/390 [==============================] - 151s 387ms/step - loss: 1.2409 - acc: 0.5607 - val_loss: 1.0959 - val_acc: 0.6172\n",
      "Epoch 8/10\n",
      "390/390 [==============================] - 152s 391ms/step - loss: 1.2008 - acc: 0.5777 - val_loss: 1.0602 - val_acc: 0.6289\n",
      "Epoch 9/10\n",
      "390/390 [==============================] - 143s 367ms/step - loss: 1.1807 - acc: 0.5840 - val_loss: 0.9390 - val_acc: 0.6635\n",
      "Epoch 10/10\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 1.1599 - acc: 0.5918 - val_loss: 0.9097 - val_acc: 0.6795\n",
      "Test loss: 0.9096961294174194\n",
      "Test accuracy: 0.6795\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(data_generator.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    steps_per_epoch=len(x_train) // batch_size, \n",
    "                    epochs=epochs,\n",
    "                    verbose=1)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "|rotation_range    |width_shift_range |height_shift_range|horizontal_flip   |Accuracy          |\n",
    "|------------------|------------------|------------------|------------------|------------------|\n",
    "|20                |0.2               |0.2               |False             |0.6208            |  \n",
    "|30                |0.1               |0.1               |False             |0.6606            |\n",
    "|30                |0.1               |0.1               |False             |0.6795            |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
