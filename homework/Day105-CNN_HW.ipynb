{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 教學目標:\n",
    "    \n",
    "回顧 CNN 網路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 範例說明:\n",
    "    \n",
    "使用 keras 預載的模型 ，可參考 [CNN 經典模型應用](https://ithelp.ithome.com.tw/articles/10192162)\n",
    "\n",
    "使用 keras VGG16 預訓練的權重\n",
    "\n",
    "了解預測後的結果輸出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業:\n",
    "\n",
    "    回答 Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "# 載入預訓練模型\n",
    "# include_top = False : produce a 512 feature maps with size of 7 x 7 pixels\n",
    "# include_top = True  : produce prediction of 1000 categories will download model file of 527 mb\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    " # VGG 現存模型要找到一張名為elephant.jpg做處理的預設路徑\n",
    "img_path = 'images/elephant.jpg'\n",
    "#載入影像\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "# generate feature maps by setting include_top = False\n",
    "features = model.predict(x)\n",
    "print(features)\n",
    "\n",
    "# do prediction by setting include_top = True\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "print('Predicted:', decode_predictions(features, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題:\n",
    "\n",
    "為什麼在CNNs中激活函數選用ReLU，而不用sigmoid或tanh函數？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "參考[卷積神經網路大總結](https://ifun01.com/B7K9FBU.html)，有以下幾個好處：\n",
    "\n",
    "- ReLU本質上是分段線性模型，前向計算非常簡單，無需指數之類操作；\n",
    "- ReLU的偏導也很簡單，反向傳播梯度，無需指數或者除法之類操作；\n",
    "- ReLU不容易發生梯度發散問題，Tanh和Logistic激活函數在兩端的時候導數容易趨近于零，多級連乘後梯度更加約等於0；\n",
    "- ReLU關閉了右邊，從而會使得很多的隱層輸出為0，即網路變得稀疏，起到了類似L1的正則化作用，可以在一定程度上緩解過擬合。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
